experiment_params:
  path: &path "/data/shared/svc/experiments"
  dataset_root: &dataset_root "/data/shared/svc/datasets/celeba_hq_256"
  dataloader_workers: &dataloader_workers 12
  image_resolution: &image_resolution (256, 256)

  gpu: &gpu 6

  lr: &lr 1e-4
  epochs: &epochs 10000
  batch_size: &batch_size 48

  scales: &scales (128, 160, 192, 192)
  n_residual_blocks: &n_residual_blocks 3
  code_book_size: &code_book_size 8192
  vq_ema_decay: &vq_ema_decay 0.8
  vq_commit_weight: &vq_commit_weight 1.

  commit_weight: &commit_weight 0.01


### Trainer
trainer: !Trainer
  deterministic: False
  benchmark: True
  gpus: `[gpu]`
  max_epochs: *epochs
  logger: !NeptuneLogger
    project: timeescaper/vq-vic
    log_model_checkpoints: True
    tags:
      - "vq"
      - "celeba_hq"
  callbacks:
    - !ModelCheckpoint
      save_top_k: 1
      save_last: True
      every_n_epochs: 1
  check_val_every_n_epoch: 5
  log_every_n_steps: 1


### Datasets
train_dataset: &train_dataset !ImagesDataset
  root_dir: *dataset_root
  segment: (0, 3500)

val_dataset: &val_dataset !ImagesDataset
  root_dir: *dataset_root
  segment: (3500, 4000)

train_dataloader: !DataLoader
  dataset: *train_dataset
  batch_size: *batch_size
  shuffle: true
  num_workers: *dataloader_workers

val_dataloader: !DataLoader
  dataset: *val_dataset
  batch_size: *batch_size
  shuffle: false
  num_workers: *dataloader_workers


# Model
model: &model !VectorQuantizedAE
  code_book_size: *code_book_size
  vq_ema_decay: *vq_ema_decay
  vq_commit_weight: *vq_commit_weight
  encoder: !ConvEncoder
    channels: *scales
    n_residual_blocks: *n_residual_blocks
  decoder: !ConvDecoder
    channels: `scales[::-1]`
    n_residual_blocks: *n_residual_blocks


### Lightning Module
pl_module: !LitAutoEncoderModule
  model: *model
  optimizer: !Adam
    lr: *lr
    params: `model.parameters()`
  loss: !DistortionLoss
    aux_weights: `{'loss_commit': commit_weight}`
  sample_train: `train_dataset[0]`
  sample_val: `val_dataset[0]`
